{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dde6e36",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "# LAG1 ‚Äî Limitless Autonomous Guardian v1\n",
    "Building a TensorFlow-native large language model that embraces multi-path reasoning, cross-thinking, and friendly NLP behaviors for the LAG project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0417c2",
   "metadata": {},
   "source": [
    "## Concept & Roadmap\n",
    "- **Mission**: deliver a chatty, coding-aware assistant that matches GPT-4 vibes while remaining open and hackable.\n",
    "- **Backbone**: decoder-only Transformer blocks enhanced with cross-thinking branches for richer deliberation.\n",
    "- **Reasoning Extras**: multi-path generation, reflection scoring, structured thought logging, controllable decoding knobs.\n",
    "- **Training Setup**: TensorFlow 2.x + mixed precision on Colab GPUs/TPUs, pipelines for custom datasets and continued-pretrain corpora.\n",
    "- **Evaluation**: perplexity, code execution harness, rubric-based human eval, and safety refusal tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6995c0f",
   "metadata": {},
   "source": [
    "## LAG1 Architecture Highlights\n",
    "1. **Dual-Stream Cross Thinking**: each decoder block maintains a primary stream (response focus) and a reflection stream (analysis), sharing information through gated cross-attention.\n",
    "2. **Thought Cache**: optional buffer that stores intermediate latent states for downstream reflection or tool use.\n",
    "3. **Reasoning Controller**: adjusts exploration parameters (temperature, top-k/p, depth) dynamically per prompt based on intent signals.\n",
    "4. **Tokenizer Flexibility**: plug-and-play between Hugging Face BPEs and lightweight `TextVectorization` for offline experimentation.\n",
    "5. **Training Modes**: supervised fine-tuning, continued pretraining, and contrastive preference optimization (hooks provided)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16b1b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: install latest dependencies when running on a fresh Colab runtime\n",
    "%pip install -q tensorflow==2.15.0 transformers datasets sentencepiece accelerate tensorflow-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c019377",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 7\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "@dataclass\n",
    "class Lag1Config:\n",
    "    vocab_size: int = 32000\n",
    "    max_position_embeddings: int = 1024\n",
    "    hidden_size: int = 768\n",
    "    num_layers: int = 12\n",
    "    num_heads: int = 12\n",
    "    cross_thinking_heads: int = 6\n",
    "    intermediate_size: int = 3072\n",
    "    dropout_rate: float = 0.1\n",
    "    layer_norm_epsilon: float = 1e-5\n",
    "    initializer_range: float = 0.02\n",
    "    pad_token_id: int = 0\n",
    "    bos_token_id: int = 1\n",
    "    eos_token_id: int = 2\n",
    "    thought_cache_size: int = 4\n",
    "    rope_theta: float = 10000.0\n",
    "    use_rotary_embeddings: bool = False\n",
    "    use_gradient_checkpointing: bool = False\n",
    "    dtype: str = \"float32\"\n",
    "\n",
    "lag_config = Lag1Config()\n",
    "print(json.dumps(asdict(lag_config), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cecb3f",
   "metadata": {},
   "source": [
    "## Data & Tokenization Pipeline\n",
    "- Supports Hugging Face tokenizers and fallback TensorFlow `TextVectorization`.\n",
    "- Provides dataset builders for conversation, code, and safety corpora with automatic mixing weights.\n",
    "- Includes utilities for prompt formatting with system/instruction/user separation and response targets.\n",
    "- Caches processed shards to Google Drive for re-use across Colab sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c78c04f",
   "metadata": {},
   "source": [
    "### Source Hugging Face datasets\n",
    "We'll fuse multiple open datasets spanning instructions, safety, math, geo, coding, and reasoning. Some require Hugging Face authentication‚Äîlog in via `huggingface-cli login` before running the loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f25326",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SPECS = [\n",
    "    {\n",
    "        \"name\": \"awesome_chatgpt_prompts\",\n",
    "        \"hf_path\": \"fka/awesome-chatgpt-prompts\",\n",
    "        \"split\": \"train\",\n",
    "        \"requires_auth\": False,\n",
    "        \"weight\": 0.1,\n",
    "        \"user_keys\": [\"prompt\"],\n",
    "        \"assistant_keys\": [\"response\", \"completion\", \"answer\"],\n",
    "        \"assistant_fallback\": lambda record, user: (\n",
    "            \"Here is a structured plan to address the following request:\\n\" + (user or \"\"))\n",
    "        ,\n",
    "        \"system_builder\": lambda record: f\"You are role-playing as {record.get('act', 'a helpful assistant')}.\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"gdpval\",\n",
    "        \"hf_path\": \"openai/gdpval\",\n",
    "        \"split\": \"train\",\n",
    "        \"requires_auth\": True,\n",
    "        \"weight\": 0.15,\n",
    "        \"user_keys\": [\"prompt\", \"question\", \"input\", \"instruction\"],\n",
    "        \"assistant_keys\": [\"ideal\", \"answer\", \"output\", \"completion\"],\n",
    "        \"system_builder\": \"You are LAG, providing careful policy-compliant answers.\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"geogpt_qa\",\n",
    "        \"hf_path\": \"GeoGPT-Research-Project/GeoGPT-QA\",\n",
    "        \"split\": \"train\",\n",
    "        \"requires_auth\": True,\n",
    "        \"weight\": 0.1,\n",
    "        \"user_keys\": [\"question\", \"Question\", \"prompt\"],\n",
    "        \"assistant_keys\": [\"answer\", \"Answer\", \"response\"],\n",
    "        \"system_builder\": \"You are LAG with deep geospatial knowledge.\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"gsm8k\",\n",
    "        \"hf_path\": \"openai/gsm8k\",\n",
    "        \"config_name\": \"main\",\n",
    "        \"split\": \"train\",\n",
    "        \"requires_auth\": False,\n",
    "        \"weight\": 0.2,\n",
    "        \"user_keys\": [\"question\"],\n",
    "        \"assistant_keys\": [\"answer\", \"solution\"],\n",
    "        \"system_builder\": \"You are LAG solving grade-school math step-by-step.\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"swe_rebench\",\n",
    "        \"hf_path\": \"nebius/SWE-rebench\",\n",
    "        \"split\": \"train\",\n",
    "        \"requires_auth\": True,\n",
    "        \"weight\": 0.2,\n",
    "        \"user_keys\": [\"prompt\", \"question\", \"instruction\", \"problem\"],\n",
    "        \"assistant_keys\": [\"solution\", \"answer\", \"completion\", \"output\"],\n",
    "        \"system_builder\": \"You are LAG helping with professional software engineering tasks.\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"math500\",\n",
    "        \"hf_path\": \"HuggingFaceH4/MATH-500\",\n",
    "        \"split\": \"train\",\n",
    "        \"requires_auth\": False,\n",
    "        \"weight\": 0.25,\n",
    "        \"user_keys\": [\"problem\", \"question\"],\n",
    "        \"assistant_keys\": [\"solution\", \"answer\"],\n",
    "        \"system_builder\": \"You are LAG solving graduate-level mathematics with detailed reasoning.\",\n",
    "    },\n",
    " ]\n",
    "\n",
    "DATASET_SPECS_BY_NAME = {spec[\"name\"]: spec for spec in DATASET_SPECS}\n",
    "\n",
    "def _first_nonempty(record: Dict[str, Any], keys: List[str]) -> Optional[str]:\n",
    "    for key in keys:\n",
    "        if key in record and record[key]:\n",
    "            value = record[key]\n",
    "            if isinstance(value, str):\n",
    "                return value.strip()\n",
    "            return json.dumps(value)\n",
    "    return None\n",
    "\n",
    "def convert_record_to_sample(source_name: str, record: Dict[str, Any]) -> Optional[Sample]:\n",
    "    spec = DATASET_SPECS_BY_NAME[source_name]\n",
    "    user = _first_nonempty(record, spec.get(\"user_keys\", []))\n",
    "    assistant = _first_nonempty(record, spec.get(\"assistant_keys\", []))\n",
    "    if assistant is None and spec.get(\"assistant_fallback\") is not None and user is not None:\n",
    "        assistant = spec[\"assistant_fallback\"](record, user)\n",
    "    if user is None or assistant is None:\n",
    "        return None\n",
    "    system_builder = spec.get(\"system_builder\")\n",
    "    if callable(system_builder):\n",
    "        system_prompt = system_builder(record)\n",
    "    elif isinstance(system_builder, str):\n",
    "        system_prompt = system_builder\n",
    "    else:\n",
    "        system_prompt = \"You are LAG, a helpful multimodal assistant.\"\n",
    "    metadata = {\n",
    "        \"source\": source_name,\n",
    "    }\n",
    "    return Sample(system=system_prompt, user=user, assistant=assistant, metadata=metadata)\n",
    "\n",
    "def load_raw_datasets(auth_token: Optional[str] = None, streaming: bool = False) -> Dict[str, Any]:\n",
    "    datasets = {}\n",
    "    for spec in DATASET_SPECS:\n",
    "        name = spec[\"name\"]\n",
    "        try:\n",
    "            args = []\n",
    "            if spec.get(\"config_name\") is not None:\n",
    "                args.append(spec[\"config_name\"])\n",
    "            load_kwargs: Dict[str, Any] = {\"split\": spec.get(\"split\", \"train\"), \"streaming\": streaming}\n",
    "            if spec.get(\"requires_auth\") and auth_token is not None:\n",
    "                load_kwargs[\"use_auth_token\"] = auth_token\n",
    "            elif spec.get(\"requires_auth\") and auth_token is None:\n",
    "                load_kwargs[\"use_auth_token\"] = True\n",
    "            dataset = load_dataset(spec[\"hf_path\"], *args, **load_kwargs)\n",
    "            datasets[name] = dataset\n",
    "            print(f\"Loaded {name} -> {spec['hf_path']} ({load_kwargs['split']})\")\n",
    "        except Exception as err:\n",
    "            print(f\"‚ö†Ô∏è Failed to load {name}: {err}\")\n",
    "    return datasets\n",
    "\n",
    "def build_weighted_tf_dataset(\n",
    "    raw_datasets: Dict[str, Any],\n",
    "    tokenizer_manager: TokenizerManager,\n",
    "    config: Lag1Config,\n",
    "    batch_size: int = 1,\n",
    "    max_per_source: Optional[int] = None,\n",
    "    weights: Optional[Dict[str, float]] = None,\n",
    "    streaming: bool = False,\n",
    ") -> tf.data.Dataset:\n",
    "    weights = weights or {spec[\"name\"]: spec.get(\"weight\", 1.0) for spec in DATASET_SPECS}\n",
    "    samples: List[Sample] = []\n",
    "    for name, dataset in raw_datasets.items():\n",
    "        spec_weight = weights.get(name, 1.0)\n",
    "        count = 0\n",
    "        iterator = dataset if streaming else dataset\n",
    "        for record in iterator:\n",
    "            sample = convert_record_to_sample(name, record)\n",
    "            if sample is None:\n",
    "                continue\n",
    "            sample.metadata = sample.metadata or {}\n",
    "            sample.metadata[\"weight\"] = spec_weight\n",
    "            samples.append(sample)\n",
    "            count += 1\n",
    "            if max_per_source is not None and count >= max_per_source:\n",
    "                break\n",
    "    if not samples:\n",
    "        raise ValueError(\"No samples could be constructed; check dataset availability and credentials.\")\n",
    "    builder = Lag1DatasetBuilder(tokenizer_manager, config)\n",
    "    if not tokenizer_manager.is_hf_tokenizer:\n",
    "        tokenizer_manager.adapt([sample.user + \" \" + sample.assistant for sample in samples])\n",
    "    return builder.build_tf_dataset(samples, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244cc423",
   "metadata": {},
   "source": [
    "> ‚ö†Ô∏è Some datasets above are gated. Authenticate with `huggingface-cli login` or create a token at https://huggingface.co/settings/tokens and pass it to `load_raw_datasets(auth_token=...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737923f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerManager:\n",
    "    \"\"\"Handles tokenizer loading or creation for LAG1.\"\"\"\n",
    "    def __init__(self, config: Lag1Config, tokenizer_name: str = \"gpt2\", vocab_size: Optional[int] = None):\n",
    "        self.config = config\n",
    "        self.tokenizer_name = tokenizer_name\n",
    "        self.vocab_size = vocab_size or config.vocab_size\n",
    "        self.tokenizer = None\n",
    "        self.is_hf_tokenizer = False\n",
    "\n",
    "    def load(self):\n",
    "        if HF_AVAILABLE:\n",
    "            print(f\"Loading Hugging Face tokenizer: {self.tokenizer_name}\")\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name)\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "            self.is_hf_tokenizer = True\n",
    "            return self.tokenizer\n",
    "\n",
    "        if tf_text is None:\n",
    "            raise RuntimeError(\"TensorFlow Text is required for fallback tokenizer creation.\")\n",
    "        print(\"Creating TensorFlow Text tokenizer from scratch.\")\n",
    "        vectorizer = tf.keras.layers.TextVectorization(\n",
    "            standardize=\"lower_and_strip_punctuation\",\n",
    "            max_tokens=self.vocab_size,\n",
    "            output_mode=\"int\",\n",
    "            output_sequence_length=self.config.max_position_embeddings,\n",
    "        )\n",
    "        self.tokenizer = vectorizer\n",
    "        self.is_hf_tokenizer = False\n",
    "        return self.tokenizer\n",
    "\n",
    "    def adapt(self, texts: List[str]):\n",
    "        if self.tokenizer is None:\n",
    "            raise ValueError(\"Tokenizer not initialized. Call load() first.\")\n",
    "        if self.is_hf_tokenizer:\n",
    "            return\n",
    "        ds = tf.data.Dataset.from_tensor_slices(texts).batch(1024)\n",
    "        self.tokenizer.adapt(ds)\n",
    "\n",
    "    def encode_batch(self, texts: List[str]) -> Dict[str, np.ndarray]:\n",
    "        if self.tokenizer is None:\n",
    "            raise ValueError(\"Tokenizer not initialized. Call load() first.\")\n",
    "\n",
    "        if self.is_hf_tokenizer:\n",
    "            encoded = self.tokenizer(\n",
    "                texts,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=self.config.max_position_embeddings,\n",
    "                return_tensors=\"np\",\n",
    "            )\n",
    "            return {\"input_ids\": encoded[\"input_ids\"], \"attention_mask\": encoded[\"attention_mask\"]}\n",
    "\n",
    "        # TensorFlow Text pathway\n",
    "        data = tf.convert_to_tensor(texts)\n",
    "        token_ids = self.tokenizer(data)\n",
    "        attention_mask = tf.cast(token_ids != 0, tf.int32)\n",
    "        return {\n",
    "            \"input_ids\": token_ids.numpy(),\n",
    "            \"attention_mask\": attention_mask.numpy(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44a6d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Sample:\n",
    "    system: str\n",
    "    user: str\n",
    "    assistant: str\n",
    "    metadata: Optional[Dict[str, Any]] = None\n",
    "\n",
    "class Lag1DatasetBuilder:\n",
    "    \"\"\"Creates TensorFlow datasets with weighted sampling from multiple sources.\"\"\"\n",
    "    def __init__(self, tokenizer_manager: TokenizerManager, config: Lag1Config):\n",
    "        self.tokenizer_manager = tokenizer_manager\n",
    "        self.config = config\n",
    "\n",
    "    def _format_prompt(self, sample: Sample) -> Tuple[str, str]:\n",
    "        system = sample.system.strip() if sample.system else \"You are LAG, a helpful AI.\"\n",
    "        prompt = f\"<s>[SYSTEM]\\n{system}\\n[/SYSTEM]\\n[USER]\\n{sample.user.strip()}\\n[/USER]\\n[ASSISTANT]\\n\"\n",
    "        target = sample.assistant.strip() + \"</s>\"\n",
    "        return prompt, target\n",
    "\n",
    "    def encode_sample(self, sample: Sample) -> Dict[str, np.ndarray]:\n",
    "        prompt, target = self._format_prompt(sample)\n",
    "        full_text = prompt + target\n",
    "        encoded = self.tokenizer_manager.encode_batch([full_text])\n",
    "        labels = encoded[\"input_ids\"].copy()\n",
    "        prompt_len = len(self.tokenizer_manager.encode_batch([prompt])[\"input_ids\"][0])\n",
    "        labels[0][:prompt_len] = -100\n",
    "        return {\n",
    "            \"input_ids\": encoded[\"input_ids\"],\n",
    "            \"attention_mask\": encoded[\"attention_mask\"],\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "\n",
    "    def build_tf_dataset(self, samples: List[Sample], batch_size: int = 1) -> tf.data.Dataset:\n",
    "        encoded_batches = [self.encode_sample(sample) for sample in samples]\n",
    "        input_ids = np.vstack([item[\"input_ids\"] for item in encoded_batches])\n",
    "        attention_masks = np.vstack([item[\"attention_mask\"] for item in encoded_batches])\n",
    "        labels = np.vstack([item[\"labels\"] for item in encoded_batches])\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((input_ids, attention_masks, labels))\n",
    "        dataset = dataset.shuffle(buffer_size=len(samples)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc57634",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_len: int, dim: int, name: Optional[str] = None):\n",
    "        super().__init__(name=name)\n",
    "        position = np.arange(max_len)[:, np.newaxis]\n",
    "        div_term = np.exp(np.arange(0, dim, 2) * -(math.log(10000.0) / dim))\n",
    "        pe = np.zeros((max_len, dim))\n",
    "        pe[:, 0::2] = np.sin(position * div_term)\n",
    "        pe[:, 1::2] = np.cos(position * div_term)\n",
    "        self.pe = tf.convert_to_tensor(pe[np.newaxis], dtype=tf.float32)\n",
    "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
    "        length = tf.shape(x)[1]\n",
    "        return tf.cast(self.pe[:, :length, :], x.dtype)\n",
    "\n",
    "def build_attention_mask(attention_mask: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"Creates a causal attention mask with padding support.\"\"\"\n",
    "    seq_len = tf.shape(attention_mask)[-1]\n",
    "    padding_mask = tf.cast(attention_mask[:, tf.newaxis, tf.newaxis, :], tf.float32)\n",
    "    causal_mask = tf.linalg.band_part(tf.ones((1, 1, seq_len, seq_len)), -1, 0)\n",
    "    return padding_mask * causal_mask\n",
    "\n",
    "class CrossThinkingBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, config: Lag1Config, name: Optional[str] = None):\n",
    "        super().__init__(name=name)\n",
    "        self.config = config\n",
    "        self.primary_self_attn = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=config.num_heads, key_dim=config.hidden_size // config.num_heads, dropout=config.dropout_rate\n",
    "        )\n",
    "        self.reflection_self_attn = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=config.cross_thinking_heads, key_dim=config.hidden_size // config.cross_thinking_heads, dropout=config.dropout_rate\n",
    "        )\n",
    "        self.primary_cross_attn = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=config.num_heads, key_dim=config.hidden_size // config.num_heads, dropout=config.dropout_rate\n",
    "        )\n",
    "        self.reflection_cross_attn = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=config.cross_thinking_heads, key_dim=config.hidden_size // config.cross_thinking_heads, dropout=config.dropout_rate\n",
    "        )\n",
    "        self.primary_mlp = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(config.intermediate_size, activation=tf.keras.activations.gelu),\n",
    "            tf.keras.layers.Dropout(config.dropout_rate),\n",
    "            tf.keras.layers.Dense(config.hidden_size),\n",
    "        ])\n",
    "        self.reflection_mlp = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(config.intermediate_size, activation=tf.keras.activations.gelu),\n",
    "            tf.keras.layers.Dropout(config.dropout_rate),\n",
    "            tf.keras.layers.Dense(config.hidden_size),\n",
    "        ])\n",
    "        self.primary_norms = [tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_epsilon) for _ in range(3)]\n",
    "        self.reflection_norms = [tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_epsilon) for _ in range(3)]\n",
    "        self.gate = tf.keras.layers.Dense(config.hidden_size, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, inputs: Dict[str, tf.Tensor], training: bool = False) -> Dict[str, tf.Tensor]:\n",
    "        primary = inputs[\"primary\"]\n",
    "        reflection = inputs[\"reflection\"]\n",
    "        mask = inputs.get(\"attention_mask\")\n",
    "        attn_mask = None\n",
    "        if mask is not None:\n",
    "            attn_mask = build_attention_mask(mask)\n",
    "\n",
    "        # Primary stream self-attention\n",
    "        p_norm = self.primary_norms[0](primary)\n",
    "        p_self = self.primary_self_attn(p_norm, p_norm, attention_mask=attn_mask, training=training)\n",
    "        primary = primary + p_self\n",
    "\n",
    "        # Reflection stream self-attention\n",
    "        r_norm = self.reflection_norms[0](reflection)\n",
    "        r_self = self.reflection_self_attn(r_norm, r_norm, attention_mask=attn_mask, training=training)\n",
    "        reflection = reflection + r_self\n",
    "\n",
    "        # Cross attention between streams\n",
    "        p_cross_norm = self.primary_norms[1](primary)\n",
    "        r_cross_norm = self.reflection_norms[1](reflection)\n",
    "        p_cross = self.primary_cross_attn(p_cross_norm, r_cross_norm, attention_mask=attn_mask, training=training)\n",
    "        r_cross = self.reflection_cross_attn(r_cross_norm, p_cross_norm, attention_mask=attn_mask, training=training)\n",
    "        primary = primary + p_cross\n",
    "        reflection = reflection + r_cross\n",
    "\n",
    "        # Feedforward\n",
    "        p_ffn = self.primary_norms[2](primary)\n",
    "        r_ffn = self.reflection_norms[2](reflection)\n",
    "        primary = primary + self.primary_mlp(p_ffn, training=training)\n",
    "        reflection = reflection + self.reflection_mlp(r_ffn, training=training)\n",
    "\n",
    "        # Gated fusion leak from reflection to primary\n",
    "        gate = self.gate(primary)\n",
    "        primary = primary + gate * reflection\n",
    "        return {\"primary\": primary, \"reflection\": reflection}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4650d07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lag1Decoder(tf.keras.Model):\n",
    "    def __init__(self, config: Lag1Config):\n",
    "        super().__init__(name=\"lag1_decoder\")\n",
    "        self.config = config\n",
    "        self.embed_tokens = tf.keras.layers.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.positional = SinusoidalPositionEmbedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout_rate)\n",
    "        self.blocks = [CrossThinkingBlock(config, name=f\"lag1_block_{i}\") for i in range(config.num_layers)]\n",
    "        self.final_norm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_epsilon)\n",
    "        self.lm_head = tf.keras.layers.Dense(config.vocab_size, use_bias=False)\n",
    "\n",
    "    def call(self, inputs: Dict[str, tf.Tensor], training: bool = False) -> Dict[str, tf.Tensor]:\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        attention_mask = inputs.get(\"attention_mask\")\n",
    "        x = self.embed_tokens(input_ids)\n",
    "        x = x + self.positional(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        primary = x\n",
    "        reflection = x\n",
    "        for block in self.blocks:\n",
    "            outputs = block({\"primary\": primary, \"reflection\": reflection, \"attention_mask\": attention_mask}, training=training)\n",
    "            primary, reflection = outputs[\"primary\"], outputs[\"reflection\"]\n",
    "\n",
    "        hidden = self.final_norm(primary)\n",
    "        logits = self.lm_head(hidden)\n",
    "        return {\"logits\": logits, \"hidden_states\": hidden, \"reflection\": reflection}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888c5757",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lag1Trainer:\n",
    "    def __init__(self, config: Lag1Config):\n",
    "        self.config = config\n",
    "        self.model = Lag1Decoder(config)\n",
    "        self.loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=\"none\")\n",
    "        self.optimizer = tf.keras.optimizers.AdamW(learning_rate=1e-4, weight_decay=0.01, beta_1=0.9, beta_2=0.95, epsilon=1e-8)\n",
    "        self.train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
    "        self.val_loss = tf.keras.metrics.Mean(name=\"val_loss\")\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, inputs: tf.Tensor, masks: tf.Tensor, labels: tf.Tensor):\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = self.model({\"input_ids\": inputs, \"attention_mask\": masks}, training=True)\n",
    "            logits = outputs[\"logits\"]\n",
    "            loss = self._compute_loss(labels, logits)\n",
    "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "        self.train_loss.update_state(loss)\n",
    "        return loss\n",
    "\n",
    "    @tf.function\n",
    "    def val_step(self, inputs: tf.Tensor, masks: tf.Tensor, labels: tf.Tensor):\n",
    "        outputs = self.model({\"input_ids\": inputs, \"attention_mask\": masks}, training=False)\n",
    "        logits = outputs[\"logits\"]\n",
    "        loss = self._compute_loss(labels, logits)\n",
    "        self.val_loss.update_state(loss)\n",
    "        return loss\n",
    "\n",
    "    def _compute_loss(self, labels: tf.Tensor, logits: tf.Tensor) -> tf.Tensor:\n",
    "        loss = self.loss_fn(labels, logits)\n",
    "        mask = tf.cast(labels != -100, tf.float32)\n",
    "        loss = tf.reduce_sum(loss * mask) / tf.reduce_sum(mask)\n",
    "        return loss\n",
    "\n",
    "    def fit(self, train_ds: tf.data.Dataset, val_ds: Optional[tf.data.Dataset] = None, epochs: int = 1, steps_per_epoch: Optional[int] = None):\n",
    "        history = {\"train_loss\": [], \"val_loss\": []}\n",
    "        for epoch in range(epochs):\n",
    "            self.train_loss.reset_state()\n",
    "            self.val_loss.reset_state()\n",
    "            for step, (inp, mask, labels) in enumerate(train_ds):\n",
    "                loss = self.train_step(inp, mask, labels)\n",
    "                if steps_per_epoch and step >= steps_per_epoch:\n",
    "                    break\n",
    "            history[\"train_loss\"].append(self.train_loss.result().numpy())\n",
    "\n",
    "            if val_ds is not None:\n",
    "                for val_inp, val_mask, val_labels in val_ds:\n",
    "                    self.val_step(val_inp, val_mask, val_labels)\n",
    "                history[\"val_loss\"].append(self.val_loss.result().numpy())\n",
    "        return history\n",
    "\n",
    "    def save(self, export_dir: str):\n",
    "        os.makedirs(export_dir, exist_ok=True)\n",
    "        dummy_inputs = {\n",
    "            \"input_ids\": tf.zeros((1, self.config.max_position_embeddings), dtype=tf.int32),\n",
    "            \"attention_mask\": tf.ones((1, self.config.max_position_embeddings), dtype=tf.int32),\n",
    "        }\n",
    "        tf.saved_model.save(self.model, export_dir, signatures=self.model.call.get_concrete_function(dummy_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6638162f",
   "metadata": {},
   "source": [
    "## Training Loop & Evaluation\n",
    "Use `Lag1Trainer` with TensorFlow `tf.data` pipelines. Hooks provided for perplexity tracking and Drive checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176135cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(logits: np.ndarray, labels: np.ndarray) -> float:\n",
    "    logits_tf = tf.convert_to_tensor(logits)\n",
    "    labels_tf = tf.convert_to_tensor(labels)\n",
    "    mask = labels_tf != -100\n",
    "    losses = tf.keras.losses.sparse_categorical_crossentropy(labels_tf, logits_tf, from_logits=True)\n",
    "    losses = tf.where(mask, losses, 0.0)\n",
    "    total_loss = tf.reduce_sum(losses)\n",
    "    token_count = tf.reduce_sum(tf.cast(mask, tf.float32))\n",
    "    ppl = tf.exp(total_loss / tf.maximum(token_count, 1.0))\n",
    "    return float(ppl.numpy())\n",
    "\n",
    "def evaluate_dataset(model: Lag1Decoder, dataset: tf.data.Dataset) -> Dict[str, float]:\n",
    "    losses = []\n",
    "    perplexities = []\n",
    "    for inputs, masks, labels in dataset:\n",
    "        outputs = model({\"input_ids\": inputs, \"attention_mask\": masks}, training=False)\n",
    "        logits = outputs[\"logits\"].numpy()\n",
    "        loss_tensor = tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "        mask = labels != -100\n",
    "        loss_tensor = tf.where(mask, loss_tensor, 0.0)\n",
    "        loss = tf.reduce_sum(loss_tensor) / tf.reduce_sum(tf.cast(mask, tf.float32))\n",
    "        losses.append(float(loss.numpy()))\n",
    "        perplexities.append(compute_perplexity(logits, labels.numpy()))\n",
    "    return {\n",
    "        \"loss\": float(np.mean(losses)) if losses else float(\"nan\"),\n",
    "        \"perplexity\": float(np.mean(perplexities)) if perplexities else float(\"nan\"),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57167b06",
   "metadata": {},
   "source": [
    "## Cross-Thinking Inference\n",
    "Generate multiple candidate continuations, score them via reflection stream alignment, and optionally perform self-reflection passes before final decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f429462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_cross_thinking(\n",
    "    model: Lag1Decoder,\n",
    "    tokenizer_manager: TokenizerManager,\n",
    "    prompt: str,\n",
    "    config: Lag1Config,\n",
    "    max_new_tokens: int = 128,\n",
    "    num_paths: int = 3,\n",
    "    temperature: float = 0.8,\n",
    "    top_k: int = 40,\n",
    "    top_p: float = 0.9,\n",
    "    reflection_strength: float = 0.3,\n",
    "    seed: Optional[int] = None,\n",
    "    thought_cache: Optional[List[Dict[str, Any]]] = None,\n",
    "    log_reflections: bool = True,\n",
    "    return_all_paths: bool = False,\n",
    "    ):\n",
    "    if seed is not None:\n",
    "        tf.keras.utils.set_random_seed(seed)\n",
    "\n",
    "    enc = tokenizer_manager.encode_batch([prompt])\n",
    "    input_ids = tf.convert_to_tensor(enc[\"input_ids\"])\n",
    "    attention_mask = tf.convert_to_tensor(enc[\"attention_mask\"])\n",
    "    generated_paths = []\n",
    "    reflection_logs = []\n",
    "\n",
    "    for path in range(num_paths):\n",
    "        cur_input_ids = tf.identity(input_ids)\n",
    "        cur_attention = tf.identity(attention_mask)\n",
    "        cur_outputs = []\n",
    "        reflections = []\n",
    "        for _ in range(max_new_tokens):\n",
    "            outputs = model({\"input_ids\": cur_input_ids, \"attention_mask\": cur_attention}, training=False)\n",
    "            logits = outputs[\"logits\"][:, -1, :] / max(temperature, 1e-5)\n",
    "            reflection_state = outputs[\"reflection\"][:, -1, :]\n",
    "            if thought_cache is not None:\n",
    "                thought_cache.append({\"reflection\": reflection_state.numpy(), \"step\": len(cur_outputs)})\n",
    "            if reflection_strength > 0:\n",
    "                reflection_logits = model.lm_head(reflection_state)\n",
    "                logits = logits + reflection_strength * reflection_logits\n",
    "            filtered_logits = top_k_top_p_filtering(logits, top_k=top_k, top_p=top_p)\n",
    "            probs = tf.nn.softmax(filtered_logits, axis=-1)\n",
    "            next_token = tf.random.categorical(tf.math.log(probs), num_samples=1)\n",
    "            token_id = int(next_token.numpy()[0][0])\n",
    "            cur_outputs.append(token_id)\n",
    "            cur_input_ids = tf.concat([cur_input_ids, next_token], axis=1)\n",
    "            next_mask = tf.ones_like(next_token)\n",
    "            cur_attention = tf.concat([cur_attention, next_mask], axis=1)\n",
    "            if token_id == config.eos_token_id:\n",
    "                break\n",
    "            if log_reflections:\n",
    "                reflections.append(reflection_state.numpy().tolist())\n",
    "        generated_paths.append(cur_outputs)\n",
    "        reflection_logs.append(reflections)\n",
    "\n",
    "    def decode(tokens: List[int]) -> str:\n",
    "        if tokenizer_manager.is_hf_tokenizer:\n",
    "            return tokenizer_manager.tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "        return \" \".join(map(str, tokens))\n",
    "\n",
    "    decoded_paths = [decode(tokens) for tokens in generated_paths]\n",
    "    if return_all_paths:\n",
    "        return {\n",
    "            \"paths\": decoded_paths,\n",
    "            \"reflection_logs\": reflection_logs,\n",
    "        }\n",
    "    best_idx = 0\n",
    "    return {\n",
    "        \"completion\": decoded_paths[best_idx],\n",
    "        \"reflection_logs\": reflection_logs[best_idx],\n",
    "        \"all_paths\": decoded_paths if log_reflections else None,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8793f832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_top_p_filtering(logits: tf.Tensor, top_k: int = 40, top_p: float = 0.9) -> tf.Tensor:\n",
    "    \"\"\"Apply top-k and top-p filtering to logits.\"\"\"\n",
    "    if top_k > 0:\n",
    "        values, _ = tf.math.top_k(logits, k=top_k)\n",
    "        min_values = values[:, -1, tf.newaxis]\n",
    "        logits = tf.where(logits < min_values, tf.fill(tf.shape(logits), -1e9), logits)\n",
    "    if top_p < 1.0:\n",
    "        sorted_logits = tf.sort(logits, direction=\"DESCENDING\", axis=-1)\n",
    "        sorted_probs = tf.nn.softmax(sorted_logits, axis=-1)\n",
    "        cumulative_probs = tf.cumsum(sorted_probs, axis=-1)\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        sorted_indices_to_remove = tf.concat([tf.zeros_like(sorted_indices_to_remove[:, :1]), sorted_indices_to_remove[:, :-1]], axis=-1)\n",
    "        sorted_logits = tf.where(sorted_indices_to_remove, tf.fill(tf.shape(sorted_logits), -1e9), sorted_logits)\n",
    "        # map back to original indices\n",
    "        sorted_indices = tf.argsort(logits, direction=\"DESCENDING\", axis=-1)\n",
    "        logits = tf.gather(sorted_logits, sorted_indices, batch_dims=1, axis=-1)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83e3767",
   "metadata": {},
   "source": [
    "## Quick CPU Smoke Test\n",
    "Utility to verify the model builds and runs on a toy batch; uses a tiny config for rapid checks in environments without GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d79489a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoke_test_lag1():\n",
    "    \"\"\"Quick CPU test to verify model builds and generates text.\"\"\"\n",
    "    print(\"üîß Setting up tiny LAG1 config for CPU smoke test...\")\n",
    "    tiny_config = Lag1Config(\n",
    "        vocab_size=1000,\n",
    "        max_position_embeddings=64,\n",
    "        hidden_size=128,\n",
    "        num_layers=2,\n",
    "        num_heads=4,\n",
    "        cross_thinking_heads=2,\n",
    "        intermediate_size=256,\n",
    "    )\n",
    "    \n",
    "    # Initialize tokenizer and model\n",
    "    tokenizer_mgr = TokenizerManager(tiny_config, tokenizer_name=\"gpt2\")\n",
    "    tokenizer_mgr.load()\n",
    "    model = Lag1Decoder(tiny_config)\n",
    "    \n",
    "    # Test forward pass\n",
    "    test_text = \"Hello, I am LAG and I can\"\n",
    "    encoded = tokenizer_mgr.encode_batch([test_text])\n",
    "    inputs = {\n",
    "        \"input_ids\": tf.convert_to_tensor(encoded[\"input_ids\"]),\n",
    "        \"attention_mask\": tf.convert_to_tensor(encoded[\"attention_mask\"])\n",
    "    }\n",
    "    \n",
    "    print(\"üöÄ Testing forward pass...\")\n",
    "    outputs = model(inputs, training=False)\n",
    "    print(f\"‚úÖ Logits shape: {outputs['logits'].shape}\")\n",
    "    print(f\"‚úÖ Reflection shape: {outputs['reflection'].shape}\")\n",
    "    \n",
    "    # Test generation\n",
    "    print(\"üéØ Testing cross-thinking generation...\")\n",
    "    result = generate_with_cross_thinking(\n",
    "        model=model,\n",
    "        tokenizer_manager=tokenizer_mgr,\n",
    "        prompt=test_text,\n",
    "        config=tiny_config,\n",
    "        max_new_tokens=20,\n",
    "        num_paths=2,\n",
    "        temperature=0.8,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    print(f\"üìù Generated completion: {result['completion']}\")\n",
    "    print(f\"üß† Reflection logs available: {len(result['reflection_logs']) > 0}\")\n",
    "    print(\"‚úÖ Smoke test passed! Model builds and generates.\")\n",
    "    \n",
    "    return model, tokenizer_mgr\n",
    "\n",
    "# Uncomment to run the smoke test\n",
    "# smoke_test_lag1()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f678c4a8",
   "metadata": {},
   "source": [
    "## Training Demo & Usage Guide\n",
    "Complete walkthrough for training LAG1 from data loading to model saving, with checkpointing and evaluation hooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455d2825",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_training_pipeline_demo():\n",
    "    \"\"\"Complete LAG1 training pipeline demonstration.\"\"\"\n",
    "    print(\"üöÄ LAG1 Training Pipeline Demo\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Step 1: Configuration\n",
    "    print(\"üîß 1. Setting up LAG1 configuration...\")\n",
    "    config = Lag1Config(\n",
    "        vocab_size=32000,\n",
    "        max_position_embeddings=512,  # Shorter for demo\n",
    "        hidden_size=768,\n",
    "        num_layers=6,  # Smaller for faster training\n",
    "        num_heads=12,\n",
    "        cross_thinking_heads=6,\n",
    "        intermediate_size=2048,\n",
    "        dropout_rate=0.1,\n",
    "    )\n",
    "    print(f\"‚úÖ Config ready: {config.num_layers} layers, {config.hidden_size}d\")\n",
    "    \n",
    "    # Step 2: Tokenizer setup\n",
    "    print(\"üî† 2. Loading tokenizer...\")\n",
    "    tokenizer_mgr = TokenizerManager(config, tokenizer_name=\"gpt2\")\n",
    "    tokenizer_mgr.load()\n",
    "    print(\"‚úÖ Tokenizer loaded\")\n",
    "    \n",
    "    # Step 3: Dataset preparation\n",
    "    print(\"üìö 3. Loading datasets...\")\n",
    "    try:\n",
    "        # Load a subset for demo\n",
    "        raw_datasets = load_raw_datasets(auth_token=None, streaming=False)\n",
    "        train_ds = build_weighted_tf_dataset(\n",
    "            raw_datasets=raw_datasets,\n",
    "            tokenizer_manager=tokenizer_mgr,\n",
    "            config=config,\n",
    "            batch_size=2,\n",
    "            max_per_source=50,  # Small demo dataset\n",
    "        )\n",
    "        print(\"‚úÖ Training dataset ready\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Dataset loading failed: {e}\")\n",
    "        print(\"Creating dummy dataset for demo...\")\n",
    "        dummy_samples = [\n",
    "            Sample(\n",
    "                system=\"You are LAG, a helpful assistant.\",\n",
    "                user=\"What is 2+2?\",\n",
    "                assistant=\"2+2 equals 4. This is a basic arithmetic operation.\",\n",
    "            ),\n",
    "            Sample(\n",
    "                system=\"You are LAG, good at coding.\",\n",
    "                user=\"Write a Python function to add two numbers.\",\n",
    "                assistant=\"def add(a, b):\\n    return a + b\\n\\nThis function takes two parameters and returns their sum.\",\n",
    "            ),\n",
    "        ]\n",
    "        builder = Lag1DatasetBuilder(tokenizer_mgr, config)\n",
    "        train_ds = builder.build_tf_dataset(dummy_samples, batch_size=1)\n",
    "    \n",
    "    # Step 4: Model and trainer setup\n",
    "    print(\"ü§ñ 4. Initializing model and trainer...\")\n",
    "    trainer = Lag1Trainer(config)\n",
    "    print(\"‚úÖ Trainer ready\")\n",
    "    \n",
    "    # Step 5: Training loop\n",
    "    print(\"üèÉ 5. Starting training (1 epoch demo)...\")\n",
    "    history = trainer.fit(\n",
    "        train_ds=train_ds,\n",
    "        val_ds=None,\n",
    "        epochs=1,\n",
    "        steps_per_epoch=5  # Very short demo\n",
    "    )\n",
    "    print(f\"‚úÖ Training complete! Final loss: {history['train_loss'][-1]:.4f}\")\n",
    "    \n",
    "    # Step 6: Evaluation\n",
    "    print(\"üìà 6. Evaluating model...\")\n",
    "    eval_metrics = evaluate_dataset(trainer.model, train_ds.take(2))\n",
    "    print(f\"‚úÖ Evaluation - Loss: {eval_metrics['loss']:.4f}, Perplexity: {eval_metrics['perplexity']:.2f}\")\n",
    "    \n",
    "    # Step 7: Generation test\n",
    "    print(\"üí¨ 7. Testing generation with cross-thinking...\")\n",
    "    test_prompt = \"Explain the concept of machine learning in simple terms.\"\n",
    "    result = generate_with_cross_thinking(\n",
    "        model=trainer.model,\n",
    "        tokenizer_manager=tokenizer_mgr,\n",
    "        prompt=test_prompt,\n",
    "        config=config,\n",
    "        max_new_tokens=50,\n",
    "        num_paths=2,\n",
    "        temperature=0.7,\n",
    "        reflection_strength=0.2,\n",
    "        seed=42\n",
    "    )\n",
    "    print(f\"üìù Generated response: {result['completion']}\")\n",
    "    \n",
    "    # Step 8: Model saving\n",
    "    print(\"üíæ 8. Saving model...\")\n",
    "    save_path = \"/tmp/lag1_demo_model\"\n",
    "    trainer.save(save_path)\n",
    "    print(f\"‚úÖ Model saved to {save_path}\")\n",
    "    \n",
    "    print(\"\\nüéâ Training pipeline demo complete!\")\n",
    "    return trainer, tokenizer_mgr, history\n",
    "\n",
    "# Uncomment to run the full demo\n",
    "# trainer, tokenizer_mgr, history = full_training_pipeline_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23fc992",
   "metadata": {},
   "source": [
    "## Advanced Reasoning Extensions\n",
    "LAG1's cross-thinking architecture enables several advanced reasoning patterns. Here are utilities for multi-step reasoning, self-correction, and thought chaining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa799c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_step_reasoning(\n",
    "    model: Lag1Decoder,\n",
    "    tokenizer_manager: TokenizerManager,\n",
    "    problem: str,\n",
    "    config: Lag1Config,\n",
    "    max_steps: int = 5,\n",
    "    step_template: str = \"Step {step}: Let me think about this...\\n\",\n",
    "    verification_template: str = \"Let me verify this step: \",\n",
    "    **generation_kwargs\n",
    "):\n",
    "    \"\"\"Implements chain-of-thought reasoning with verification steps.\"\"\"\n",
    "    reasoning_chain = []\n",
    "    current_context = problem\n",
    "    \n",
    "    for step in range(1, max_steps + 1):\n",
    "        # Generate reasoning step\n",
    "        step_prompt = current_context + \"\\n\" + step_template.format(step=step)\n",
    "        step_result = generate_with_cross_thinking(\n",
    "            model=model,\n",
    "            tokenizer_manager=tokenizer_manager,\n",
    "            prompt=step_prompt,\n",
    "            config=config,\n",
    "            max_new_tokens=100,\n",
    "            reflection_strength=0.4,  # Higher reflection for reasoning\n",
    "            **generation_kwargs\n",
    "        )\n",
    "        \n",
    "        step_reasoning = step_result['completion']\n",
    "        reasoning_chain.append({\n",
    "            \"step\": step,\n",
    "            \"reasoning\": step_reasoning,\n",
    "            \"reflection_log\": step_result['reflection_logs']\n",
    "        })\n",
    "        \n",
    "        # Self-verification step\n",
    "        verify_prompt = step_prompt + step_reasoning + \"\\n\" + verification_template\n",
    "        verify_result = generate_with_cross_thinking(\n",
    "            model=model,\n",
    "            tokenizer_manager=tokenizer_manager,\n",
    "            prompt=verify_prompt,\n",
    "            config=config,\n",
    "            max_new_tokens=50,\n",
    "            reflection_strength=0.6,  # Even higher for verification\n",
    "            **generation_kwargs\n",
    "        )\n",
    "        \n",
    "        reasoning_chain[-1]['verification'] = verify_result['completion']\n",
    "        current_context = step_prompt + step_reasoning + \"\\n\" + verify_result['completion']\n",
    "        \n",
    "        # Check if reasoning is complete\n",
    "        if any(keyword in step_reasoning.lower() for keyword in ['therefore', 'conclusion', 'final answer']):\n",
    "            break\n",
    "    \n",
    "    return {\n",
    "        \"problem\": problem,\n",
    "        \"reasoning_chain\": reasoning_chain,\n",
    "        \"final_context\": current_context\n",
    "    }\n",
    "\n",
    "def self_correction_loop(\n",
    "    model: Lag1Decoder,\n",
    "    tokenizer_manager: TokenizerManager,\n",
    "    initial_answer: str,\n",
    "    question: str,\n",
    "    config: Lag1Config,\n",
    "    max_corrections: int = 3,\n",
    "    **generation_kwargs\n",
    "):\n",
    "    \"\"\"Implements self-correction by having the model critique and improve its own answers.\"\"\"\n",
    "    corrections = []\n",
    "    current_answer = initial_answer\n",
    "    \n",
    "    for correction_round in range(max_corrections):\n",
    "        # Generate critique\n",
    "        critique_prompt = f\"Question: {question}\\nAnswer: {current_answer}\\n\\nCritique this answer and identify any errors or improvements:\"\n",
    "        critique_result = generate_with_cross_thinking(\n",
    "            model=model,\n",
    "            tokenizer_manager=tokenizer_manager,\n",
    "            prompt=critique_prompt,\n",
    "            config=config,\n",
    "            max_new_tokens=150,\n",
    "            reflection_strength=0.7,\n",
    "            **generation_kwargs\n",
    "        )\n",
    "        \n",
    "        critique = critique_result['completion']\n",
    "        \n",
    "        # Generate improved answer\n",
    "        improve_prompt = f\"Question: {question}\\nPrevious answer: {current_answer}\\nCritique: {critique}\\n\\nProvide an improved answer:\"\n",
    "        improved_result = generate_with_cross_thinking(\n",
    "            model=model,\n",
    "            tokenizer_manager=tokenizer_manager,\n",
    "            prompt=improve_prompt,\n",
    "            config=config,\n",
    "            max_new_tokens=200,\n",
    "            reflection_strength=0.5,\n",
    "            **generation_kwargs\n",
    "        )\n",
    "        \n",
    "        improved_answer = improved_result['completion']\n",
    "        \n",
    "        corrections.append({\n",
    "            \"round\": correction_round + 1,\n",
    "            \"critique\": critique,\n",
    "            \"improved_answer\": improved_answer,\n",
    "            \"critique_reflections\": critique_result['reflection_logs'],\n",
    "            \"improvement_reflections\": improved_result['reflection_logs']\n",
    "        })\n",
    "        \n",
    "        # Check if significant improvement was made\n",
    "        if len(improved_answer) < len(current_answer) * 1.1:  # Simple heuristic\n",
    "            break\n",
    "            \n",
    "        current_answer = improved_answer\n",
    "    \n",
    "    return {\n",
    "        \"initial_answer\": initial_answer,\n",
    "        \"final_answer\": current_answer,\n",
    "        \"corrections\": corrections\n",
    "    }\n",
    "\n",
    "def thought_tree_exploration(\n",
    "    model: Lag1Decoder,\n",
    "    tokenizer_manager: TokenizerManager,\n",
    "    problem: str,\n",
    "    config: Lag1Config,\n",
    "    branch_factor: int = 3,\n",
    "    max_depth: int = 3,\n",
    "    **generation_kwargs\n",
    "):\n",
    "    \"\"\"Explores multiple reasoning paths in a tree structure.\"\"\"\n",
    "    from collections import deque\n",
    "    \n",
    "    # Initialize tree with root problem\n",
    "    tree = {\n",
    "        \"problem\": problem,\n",
    "        \"branches\": [],\n",
    "        \"depth\": 0\n",
    "    }\n",
    "    \n",
    "    queue = deque([tree])\n",
    "    \n",
    "    while queue:\n",
    "        current_node = queue.popleft()\n",
    "        \n",
    "        if current_node[\"depth\"] >= max_depth:\n",
    "            continue\n",
    "            \n",
    "        # Generate multiple reasoning branches\n",
    "        for branch_idx in range(branch_factor):\n",
    "            branch_prompt = current_node[\"problem\"] + f\"\\n\\nApproach {branch_idx + 1}: \"\n",
    "            branch_result = generate_with_cross_thinking(\n",
    "                model=model,\n",
    "                tokenizer_manager=tokenizer_manager,\n",
    "                prompt=branch_prompt,\n",
    "                config=config,\n",
    "                max_new_tokens=80,\n",
    "                seed=42 + branch_idx,  # Different seeds for diversity\n",
    "                **generation_kwargs\n",
    "            )\n",
    "            \n",
    "            branch_node = {\n",
    "                \"approach\": branch_idx + 1,\n",
    "                \"reasoning\": branch_result['completion'],\n",
    "                \"problem\": branch_prompt + branch_result['completion'],\n",
    "                \"depth\": current_node[\"depth\"] + 1,\n",
    "                \"branches\": [],\n",
    "                \"reflections\": branch_result['reflection_logs']\n",
    "            }\n",
    "            \n",
    "            current_node[\"branches\"].append(branch_node)\n",
    "            queue.append(branch_node)\n",
    "    \n",
    "    return tree\n",
    "\n",
    "def consensus_reasoning(\n",
    "    model: Lag1Decoder,\n",
    "    tokenizer_manager: TokenizerManager,\n",
    "    problem: str,\n",
    "    config: Lag1Config,\n",
    "    num_agents: int = 5,\n",
    "    **generation_kwargs\n",
    "):\n",
    "    \"\"\"Generates multiple independent solutions and finds consensus.\"\"\"\n",
    "    agent_solutions = []\n",
    "    \n",
    "    for agent_id in range(num_agents):\n",
    "        agent_prompt = f\"Agent {agent_id + 1}, solve this problem: {problem}\"\n",
    "        solution = generate_with_cross_thinking(\n",
    "            model=model,\n",
    "            tokenizer_manager=tokenizer_manager,\n",
    "            prompt=agent_prompt,\n",
    "            config=config,\n",
    "            seed=42 + agent_id,\n",
    "            **generation_kwargs\n",
    "        )\n",
    "        \n",
    "        agent_solutions.append({\n",
    "            \"agent_id\": agent_id + 1,\n",
    "            \"solution\": solution['completion'],\n",
    "            \"reflections\": solution['reflection_logs']\n",
    "        })\n",
    "    \n",
    "    # Generate consensus\n",
    "    solutions_text = \"\\n\\n\".join([f\"Agent {sol['agent_id']}: {sol['solution']}\" for sol in agent_solutions])\n",
    "    consensus_prompt = f\"Problem: {problem}\\n\\nMultiple solutions:\\n{solutions_text}\\n\\nAnalyze these solutions and provide a consensus answer:\"\n",
    "    \n",
    "    consensus_result = generate_with_cross_thinking(\n",
    "        model=model,\n",
    "        tokenizer_manager=tokenizer_manager,\n",
    "        prompt=consensus_prompt,\n",
    "        config=config,\n",
    "        max_new_tokens=200,\n",
    "        reflection_strength=0.8,\n",
    "        **generation_kwargs\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"problem\": problem,\n",
    "        \"agent_solutions\": agent_solutions,\n",
    "        \"consensus\": consensus_result['completion'],\n",
    "        \"consensus_reflections\": consensus_result['reflection_logs']\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a85b94",
   "metadata": {},
   "source": [
    "## Production Deployment Utilities\n",
    "Helper functions for converting trained LAG1 models to production-ready formats and setting up inference servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca698524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inference_server_code(model_path: str, tokenizer_name: str = \"gpt2\"):\n",
    "    \"\"\"Generate FastAPI server code for LAG1 inference.\"\"\"\n",
    "    server_code = f'''\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from typing import Optional, List, Dict, Any\n",
    "\n",
    "# Import LAG1 components (assuming they're in a module)\n",
    "from lag1_model import Lag1Config, Lag1Decoder, TokenizerManager, generate_with_cross_thinking\n",
    "\n",
    "app = FastAPI(title=\"LAG1 Inference Server\", version=\"1.0.0\")\n",
    "\n",
    "# Load model and tokenizer at startup\n",
    "config = Lag1Config()  # Load from saved config\n",
    "model = tf.saved_model.load(\"{model_path}\")\n",
    "tokenizer_mgr = TokenizerManager(config, tokenizer_name=\"{tokenizer_name}\")\n",
    "tokenizer_mgr.load()\n",
    "\n",
    "class GenerationRequest(BaseModel):\n",
    "    prompt: str\n",
    "    max_new_tokens: int = 128\n",
    "    temperature: float = 0.8\n",
    "    top_k: int = 40\n",
    "    top_p: float = 0.9\n",
    "    num_paths: int = 1\n",
    "    reflection_strength: float = 0.3\n",
    "    use_cross_thinking: bool = True\n",
    "    seed: Optional[int] = None\n",
    "\n",
    "class GenerationResponse(BaseModel):\n",
    "    completion: str\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "@app.post(\"/generate\", response_model=GenerationResponse)\n",
    "async def generate_text(request: GenerationRequest):\n",
    "    try:\n",
    "        if request.use_cross_thinking:\n",
    "            result = generate_with_cross_thinking(\n",
    "                model=model,\n",
    "                tokenizer_manager=tokenizer_mgr,\n",
    "                prompt=request.prompt,\n",
    "                config=config,\n",
    "                max_new_tokens=request.max_new_tokens,\n",
    "                temperature=request.temperature,\n",
    "                top_k=request.top_k,\n",
    "                top_p=request.top_p,\n",
    "                num_paths=request.num_paths,\n",
    "                reflection_strength=request.reflection_strength,\n",
    "                seed=request.seed\n",
    "            )\n",
    "            return GenerationResponse(\n",
    "                completion=result[\"completion\"],\n",
    "                metadata={{\n",
    "                    \"reflection_logs_available\": len(result[\"reflection_logs\"]) > 0,\n",
    "                    \"num_reflection_steps\": len(result[\"reflection_logs\"])\n",
    "                }}\n",
    "            )\n",
    "        else:\n",
    "            # Standard generation without cross-thinking\n",
    "            # Implementation would go here\n",
    "            raise HTTPException(status_code=501, detail=\"Standard generation not implemented\")\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    return {{\"status\": \"healthy\", \"model_loaded\": model is not None}}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "'''\n",
    "    return server_code\n",
    "\n",
    "def export_for_deployment(\n",
    "    trainer: Lag1Trainer,\n",
    "    tokenizer_manager: TokenizerManager,\n",
    "    export_dir: str,\n",
    "    include_server: bool = True\n",
    "):\n",
    "    \"\"\"Export trained LAG1 model for production deployment.\"\"\"\n",
    "    import os\n",
    "    os.makedirs(export_dir, exist_ok=True)\n",
    "    \n",
    "    # Save model\n",
    "    model_path = os.path.join(export_dir, \"model\")\n",
    "    trainer.save(model_path)\n",
    "    print(f\"‚úÖ Model saved to {model_path}\")\n",
    "    \n",
    "    # Save config\n",
    "    config_path = os.path.join(export_dir, \"config.json\")\n",
    "    with open(config_path, \"w\") as f:\n",
    "        json.dump(asdict(trainer.config), f, indent=2)\n",
    "    print(f\"‚úÖ Config saved to {config_path}\")\n",
    "    \n",
    "    # Save tokenizer info\n",
    "    tokenizer_info = {\n",
    "        \"tokenizer_name\": tokenizer_manager.tokenizer_name,\n",
    "        \"vocab_size\": tokenizer_manager.vocab_size,\n",
    "        \"is_hf_tokenizer\": tokenizer_manager.is_hf_tokenizer\n",
    "    }\n",
    "    tokenizer_path = os.path.join(export_dir, \"tokenizer_info.json\")\n",
    "    with open(tokenizer_path, \"w\") as f:\n",
    "        json.dump(tokenizer_info, f, indent=2)\n",
    "    print(f\"‚úÖ Tokenizer info saved to {tokenizer_path}\")\n",
    "    \n",
    "    if include_server:\n",
    "        # Generate server code\n",
    "        server_code = create_inference_server_code(model_path, tokenizer_manager.tokenizer_name)\n",
    "        server_path = os.path.join(export_dir, \"server.py\")\n",
    "        with open(server_path, \"w\") as f:\n",
    "            f.write(server_code)\n",
    "        print(f\"‚úÖ Server code generated at {server_path}\")\n",
    "        \n",
    "        # Generate requirements.txt\n",
    "        requirements = [\n",
    "            \"tensorflow>=2.15.0\",\n",
    "            \"transformers>=4.21.0\",\n",
    "            \"fastapi>=0.100.0\",\n",
    "            \"uvicorn>=0.23.0\",\n",
    "            \"pydantic>=2.0.0\",\n",
    "            \"datasets>=2.14.0\"\n",
    "        ]\n",
    "        req_path = os.path.join(export_dir, \"requirements.txt\")\n",
    "        with open(req_path, \"w\") as f:\n",
    "            f.write(\"\\n\".join(requirements))\n",
    "        print(f\"‚úÖ Requirements saved to {req_path}\")\n",
    "        \n",
    "        # Generate deployment instructions\n",
    "        deploy_instructions = f'''\n",
    "# LAG1 Deployment Instructions\n",
    "\n",
    "## Setup\n",
    "1. Install dependencies:\n",
    "   ```bash\n",
    "   pip install -r requirements.txt\n",
    "   ```\n",
    "\n",
    "2. Ensure the LAG1 model components are available (copy from notebook).\n",
    "\n",
    "## Running the Server\n",
    "```bash\n",
    "python server.py\n",
    "```\n",
    "\n",
    "## Testing\n",
    "```bash\n",
    "curl -X POST \"http://localhost:8000/generate\" \\\\\n",
    "     -H \"Content-Type: application/json\" \\\\\n",
    "     -d '{{\n",
    "       \"prompt\": \"Explain quantum computing\",\n",
    "       \"max_new_tokens\": 100,\n",
    "       \"temperature\": 0.7,\n",
    "       \"use_cross_thinking\": true\n",
    "     }}'\n",
    "```\n",
    "\n",
    "## Health Check\n",
    "```bash\n",
    "curl http://localhost:8000/health\n",
    "```\n",
    "'''\n",
    "        deploy_path = os.path.join(export_dir, \"README_DEPLOYMENT.md\")\n",
    "        with open(deploy_path, \"w\") as f:\n",
    "            f.write(deploy_instructions)\n",
    "        print(f\"‚úÖ Deployment instructions saved to {deploy_path}\")\n",
    "    \n",
    "    print(f\"\\nüéâ Deployment package ready in {export_dir}\")\n",
    "    return export_dir\n",
    "\n",
    "def benchmark_model(\n",
    "    model: Lag1Decoder,\n",
    "    tokenizer_manager: TokenizerManager,\n",
    "    config: Lag1Config,\n",
    "    test_prompts: List[str],\n",
    "    num_runs: int = 5\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Benchmark LAG1 model performance.\"\"\"\n",
    "    import time\n",
    "    import statistics\n",
    "    \n",
    "    results = {\n",
    "        \"total_prompts\": len(test_prompts),\n",
    "        \"num_runs\": num_runs,\n",
    "        \"per_prompt_stats\": [],\n",
    "        \"overall_stats\": {}\n",
    "    }\n",
    "    \n",
    "    all_times = []\n",
    "    all_tokens_per_sec = []\n",
    "    \n",
    "    for prompt in test_prompts:\n",
    "        prompt_times = []\n",
    "        for run in range(num_runs):\n",
    "            start_time = time.time()\n",
    "            result = generate_with_cross_thinking(\n",
    "                model=model,\n",
    "                tokenizer_manager=tokenizer_manager,\n",
    "                prompt=prompt,\n",
    "                config=config,\n",
    "                max_new_tokens=50,\n",
    "                num_paths=1\n",
    "            )\n",
    "            end_time = time.time()\n",
    "            \n",
    "            generation_time = end_time - start_time\n",
    "            tokens_generated = len(tokenizer_manager.encode_batch([result['completion']])[\"input_ids\"][0])\n",
    "            tokens_per_sec = tokens_generated / generation_time if generation_time > 0 else 0\n",
    "            \n",
    "            prompt_times.append(generation_time)\n",
    "            all_times.append(generation_time)\n",
    "            all_tokens_per_sec.append(tokens_per_sec)\n",
    "        \n",
    "        results[\"per_prompt_stats\"].append({\n",
    "            \"prompt\": prompt[:50] + \"...\" if len(prompt) > 50 else prompt,\n",
    "            \"avg_time\": statistics.mean(prompt_times),\n",
    "            \"min_time\": min(prompt_times),\n",
    "            \"max_time\": max(prompt_times),\n",
    "            \"std_time\": statistics.stdev(prompt_times) if len(prompt_times) > 1 else 0\n",
    "        })\n",
    "    \n",
    "    results[\"overall_stats\"] = {\n",
    "        \"avg_generation_time\": statistics.mean(all_times),\n",
    "        \"avg_tokens_per_sec\": statistics.mean(all_tokens_per_sec),\n",
    "        \"min_generation_time\": min(all_times),\n",
    "        \"max_generation_time\": max(all_times),\n",
    "        \"std_generation_time\": statistics.stdev(all_times) if len(all_times) > 1 else 0\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aed145e",
   "metadata": {},
   "source": [
    "## üöÄ Getting Started with LAG1\n",
    "\n",
    "### Quick Start\n",
    "1. **Run the smoke test** to verify everything works:\n",
    "   ```python\n",
    "   smoke_test_lag1()\n",
    "   ```\n",
    "\n",
    "2. **Load your datasets** (authenticate for gated ones):\n",
    "   ```python\n",
    "   raw_datasets = load_raw_datasets(auth_token=None)  # or your HF token\n",
    "   ```\n",
    "\n",
    "3. **Train a model**:\n",
    "   ```python\n",
    "   full_training_pipeline_demo()\n",
    "   ```\n",
    "\n",
    "### Advanced Features\n",
    "\n",
    "**Multi-Step Reasoning**: Chain thoughts with verification\n",
    "```python\n",
    "reasoning_result = multi_step_reasoning(model, tokenizer_mgr, \"Solve 2x + 5 = 13\", config)\n",
    "```\n",
    "\n",
    "**Self-Correction**: Let the model improve its own answers\n",
    "```python\n",
    "corrected = self_correction_loop(model, tokenizer_mgr, initial_answer, question, config)\n",
    "```\n",
    "\n",
    "**Consensus Reasoning**: Multiple agents collaborate\n",
    "```python\n",
    "consensus = consensus_reasoning(model, tokenizer_mgr, \"Explain photosynthesis\", config)\n",
    "```\n",
    "\n",
    "### Production Deployment\n",
    "```python\n",
    "# Export trained model for production\n",
    "export_for_deployment(trainer, tokenizer_mgr, \"/path/to/export\", include_server=True)\n",
    "\n",
    "# Benchmark performance\n",
    "test_prompts = [\"Hello, how are you?\", \"Explain AI\", \"Write Python code\"]\n",
    "benchmark_results = benchmark_model(model, tokenizer_mgr, config, test_prompts)\n",
    "```\n",
    "\n",
    "### Key Features\n",
    "- **üß† Cross-Thinking Architecture**: Dual-stream reasoning with primary and reflection paths\n",
    "- **üîÑ Multi-Path Generation**: Explore multiple reasoning approaches simultaneously\n",
    "- **üõ†Ô∏è Flexible Tokenization**: Works with Hugging Face tokenizers or TensorFlow Text\n",
    "- **üìä Comprehensive Datasets**: Supports 6 diverse datasets for training\n",
    "- **üöÄ Production Ready**: Includes FastAPI server generation and deployment utilities\n",
    "- **üîß Advanced Reasoning**: Chain-of-thought, self-correction, consensus mechanisms\n",
    "\n",
    "### Next Steps\n",
    "1. **Experiment with hyperparameters** in `Lag1Config`\n",
    "2. **Add custom datasets** by extending `DATASET_SPECS`\n",
    "3. **Implement new reasoning patterns** using the cross-thinking utilities\n",
    "4. **Scale up training** on larger datasets and longer sequences\n",
    "5. **Deploy to production** using the generated FastAPI server\n",
    "\n",
    "### Troubleshooting\n",
    "- **Authentication errors**: Run `huggingface-cli login` or provide a valid token\n",
    "- **Memory issues**: Reduce `batch_size`, `max_position_embeddings`, or `num_layers`\n",
    "- **Slow training**: Enable mixed precision, use TPUs on Colab, or implement gradient accumulation\n",
    "- **Poor generation quality**: Increase model size, train longer, or adjust generation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b98d056",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_len: int, dim: int, name: Optional[str] = None):\n",
    "        super().__init__(name=name)\n",
    "        position = np.arange(max_len)[:, np.newaxis]\n",
    "        div_term = np.exp(np.arange(0, dim, 2) * -(math.log(10000.0) / dim))\n",
    "        self.pe = np.zeros((max_len, dim))\n",
    "        self.pe[:, 0::2] = np.sin(position * div_term)\n",
    "        self.pe[:, 1::2] = np.cos(position * div_term)\n",
    "        self.pe = self.pe[np.newaxis]\n",
    "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
    "        length = tf.shape(x)[1]\n",
    "        return tf.cast(self.pe[:, :length, :], x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016b5fe5",
   "metadata": {},
   "source": [
    "## Multi-Stream Decoder Architecture\n",
    "The cross-thinking block runs primary and reflection streams in parallel, exchanging context via cross-attention and gated fusion. Rotary or learned position embeddings keep both streams aligned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ba6c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datasets import load_dataset\n",
    "try:\n",
    "    import tensorflow_text as tf_text  # noqa: F401\n",
    "except ModuleNotFoundError:\n",
    "    tf_text = None\n",
    "try:\n",
    "    from transformers import AutoTokenizer\n",
    "    HF_AVAILABLE = True\n",
    "except ModuleNotFoundError:\n",
    "    AutoTokenizer = None\n",
    "    HF_AVAILABLE = False\n",
    "print({\n",
    "    \"tensorflow_version\": tf.__version__,\n",
    "    \"eager_execution\": tf.executing_eagerly(),\n",
    "    \"tf_text_available\": tf_text is not None,\n",
    "    \"transformers_available\": HF_AVAILABLE,\n",
    "    \"gpu_available\": tf.config.list_physical_devices(\"GPU\")\n",
    "})\n",
    "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
